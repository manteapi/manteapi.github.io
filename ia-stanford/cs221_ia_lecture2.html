<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Meta data -->
        <!-- Encoding -->
        <meta http-equiv="Content-Type" content="text/HTML; charset=utf-8">
        <!-- Viewport adptation to device -->
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <!-- Keywords for search engine -->
        <meta name="keywords" content="artificial intelligence, course notes">
        <!-- Description of the webpage -->
        <meta name="description" content="Notes about artificial intelligence">
        <!-- Author -->
        <meta name="author" content="Pierre-Luc Manteaux">

        <!-- En-tÃªte de la page -->
        <title>AI Course Notes - Lecture - Pierre-Luc Manteaux</title>

        <!-- <link rel="stylesheet" href="style.css"/> -->
        <link rel="stylesheet" href="./../style.css"> 

        <!-- I love MathJax -->
        <!-- <script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>-->
        <!-- But MathJax + PSTricks is better -->
        <script type="text/x-mathjax-config">
// <![CDATA[
    MathJax.Hub.Config({ 
        TeX: {extensions: ["AMSmath.js", "AMSsymbols.js"]},     
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        showProcessingMessages : false,
        messageStyle : "none" ,    
        showMathMenu: false ,
        tex2jax: {
            processEnvironments: true,
            inlineMath: [ ['$','$'] ],
            displayMath: [ ['$$','$$'], ["\[","\]"] ],
            preview : "none",
            processEscapes: true
        },
        "HTML-CSS": { linebreaks: { automatic:true, width: "latex-container"} }
    });
// ]]>
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

        <!--Highlight-->
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/styles/default.min.css">
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>

        <!--Bootstrap & JQuery library-->
        <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
        <script src="https://code.jquery.com/jquery-1.11.2.min.js"></script>
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>

        <!--Html scripts-->
        <script type="text/javascript" src="./../website.js"></script>
    </head>

    <body class="website course">
        <div class="container">
            <div class="row">
                <script type="text/javascript">
                    document.write(website.header('./../'));
                </script>
                <section class="col-md-12">
                    <header>
                        <h1>Artificial Intelligence - Lecture 2 - Linear classification, loss minimization, stochastic gradient descent</h1>
                        <hr>
                    </header>
                </section>
                <main>
                    <article>
                        <section class="col-md-12">
                            <header>
                                <h2>Scattered notes</h2>
                            </header>
                            <h3>Predictor</h3>
                            <blockquote>
                                <p>
                                    A <strong>predictor</strong> is a function $f$ that maps an input $x$ to an output $y$. We can distinguish <strong>classifiers</strong> which have discrete outputs from <strong>regression</strong> which have continous outpus.
                                </p>
                            </blockquote>
                            <p>Examples of discrete predictions : </p>
                            <ul>
                                <li>
                                    <strong>Multiclass classification</strong>
                                    <p>
                                        The output is a category. For instance, given an image, what is the main content ? A face, A cat, A dog, ... ?
                                    </p>
                                </li>
                                <li>
                                    <strong>Ranking</strong>
                                    <p>
                                        The output is a permutation of the input which corresponds to a given ranking rule.
                                    </p>
                                </li>
                                <li>
                                    <strong>Structured prediction</strong>
                                    <p>
                                        The output is built from parts. For instance the traduction of a sentence in another language.
                                    </p>
                                </li>
                            </ul>
                            <p>
                                <strong>Supervised learning</strong> : The data used for the prediction task provides both inputs and outpus, in contrast with <strong>unsupervised learning</strong> wich only provides inputs.
                            </p>
                            <p>
                                <strong>Example</strong> : A input-output pair $(x,y)$ which specifies that the $y$ is the ground-truth output for $x$. 
                            </p>
                            <p>
                                <strong>Training data</strong> : A multiset of examples which specify the desired behavior of the predictor.
                            </p>
                            <p>
                                <strong>Learning</strong> : Producing a predictor $f$ from the training data.  $f$ should work with other data than the training ones.
                            </p>
                            <h4>Binary feature-based predictor</h4>
                            <p><strong>Feature</strong> : Properties of $x$ which might be relevant for predicting $y$.</p>
                            <p><strong>Feature vector</strong> : $\phi(x)$ is computed by the feature extractor $\phi(x)=[\phi_{1}(x),\cdots,\phi_{d}(x)]$.</p>
                            <p><strong>Weight vector</strong> : $w=[w_{1},\cdots,w_{d}]$ specifies the weight of each feature to the prediction.</p>
                            <p><strong>Score</strong> : Weighted sum of the features. It represents how <strong>confident</strong> we are in predicting $x$.</p>
                            $$\displaystyle w \cdot \phi(x) = \sum_{i}^{d}w_{i}\phi_{i}(x)$$
                            <p><stong>Binary linear classifier</stong></p>
                            $$
                            f_{w}(x) = \text{sign}(w \cdot \phi(x)) = 
                            \left\lbrace
                            \begin{array}{lll}
                            +1 & \text{if} & w \cdot \phi(x) &gt; 0 \\
                            -1 & \text{if} & w \cdot \phi(x) &lt; 0 \\
                            \text{?} & \text{if} & w \cdot \phi(x) = 0 \\
                            \end{array}
                            \right.
                            $$
                            <blockquote>
                                <p>$f_{w}$ defines a hyperplane with normal vector $w$.</p>
                            </blockquote>
                            <h3>Loss minimization - How to learn $w$ from training ?</h3>
                            <p>
                                <strong>Loss minimization</strong> is a framework which allows to cast learning as an <strong>optimization problem</strong> that will be solved using an <strong>optimization algorithm</strong>.
                            </p>
                            <p>
                                <strong>Loss function</strong> : $\text{Loss}(x,y,w)$ quantifies how bad it would be to use $w$ to make a prediction on $x$ when the output is $y$.
                            </p>
                            <p>
                                <strong>Margin</strong> : $(w\cdot \phi(x))_{y}$ represents how <strong>correct</strong> we are.
                            </p>
                            <p>
                                <strong>Zero-one loss</strong> : A loss function which represents if the predictor has made a mistake or not.
                                $$
                                \begin{array}{lll}
                                \text{Loss}_{0-1}(x,y,w) & = & \mathbf{1}[f_{w}(x)\neq y] \\
                                & = & \mathbf{1}[(w \cdot \phi(x))y \leq 0]
                                \end{array}
                                $$
                            </p>
                            <p>
                                <strong>Residual</strong> : If the output was real instead of binary, the residual would measure how close the prediction if to the correct output : $w \cdot \phi(x) - y$. 
                            </p>
                            <p>
                                <strong>Squared loss</strong> : 
                            </p>
                            $$
                            \text{Loss}_{\text{squared}(x,y,w) = \left(f_{w}(x)-y\right)^{2}
                            $$
                            <p>
                                <strong>Absolute deviation loss</strong> : 
                            </p>
                            $$
                            \text{Loss}_{\text{squared}(x,y,w) = \parallel f_{w}(x)-y\parallel
                            $$
                            <p>
                                <strong>Training loss</strong>
                            </p>
                        </section>
                    </article>
                </main>
            </div>
            <div class="row">
                <footer>
                    <hr>
                    <a href="http://jigsaw.w3.org/css-validator/check/referer"><img style="border:0;width:88px;height:31px" src="http://jigsaw.w3.org/css-validator/images/vcss-blue" alt="Valid CSS!" /></a>
                    <a href="http://www.mathjax.org"><img title="Powered by MathJax" src="http://cdn.mathjax.org/mathjax/badge/badge.gif" alt="Powered by MathJax" /></a>
                    <address>by <a href="mailto:frajus@hotmail.fr">Frajus</a></address>
                </footer>
            </div>
        </div>             
    </body>
</html>